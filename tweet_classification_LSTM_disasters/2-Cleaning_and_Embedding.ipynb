{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "In this notebook the raw text data is cleaned and word embeddings are generated with an LSTM.\n",
    "\n",
    "**Data Sources:**\n",
    "\n",
    "- `data/raw/train.csv`: labelled data for approx. 7'600 tweets\n",
    "- `data/raw/test.csv`: test data for approx. 3'400 tweets\n",
    "\n",
    "**Data Output:**\n",
    "\n",
    "- `xxx.csv`: blablabla\n",
    "\n",
    "**Changes**\n",
    "\n",
    "- 2020-01-07: Start notebook, complete preprocessing\n",
    "\n",
    "___\n",
    "\n",
    "_**Open Questions / Things to check out for the future:**_\n",
    "- Import pre-trained embedding (FastText, GloVe), see this [MLM post](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/) and the appendix\n",
    "- Preprocessing with removal of stop-words and / or punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries,-load-data\" data-toc-modified-id=\"Import-libraries,-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import libraries, load data</a></span></li><li><span><a href=\"#Preprocess-text\" data-toc-modified-id=\"Preprocess-text-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preprocess text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Clean\" data-toc-modified-id=\"Clean-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Clean</a></span></li><li><span><a href=\"#Encode\" data-toc-modified-id=\"Encode-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Encode</a></span></li><li><span><a href=\"#Subsample\" data-toc-modified-id=\"Subsample-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Subsample</a></span></li><li><span><a href=\"#Create-batches¶\" data-toc-modified-id=\"Create-batches¶-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Create batches¶</a></span></li></ul></li><li><span><a href=\"#Build-NN-for-Word2Vec-Embedding\" data-toc-modified-id=\"Build-NN-for-Word2Vec-Embedding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build NN for Word2Vec Embedding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Model-Architecture-and-Loss-Function\" data-toc-modified-id=\"Define-Model-Architecture-and-Loss-Function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Define Model Architecture and Loss Function</a></span></li><li><span><a href=\"#Define-validation-function\" data-toc-modified-id=\"Define-validation-function-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Define validation function</a></span></li></ul></li><li><span><a href=\"#Training-NN\" data-toc-modified-id=\"Training-NN-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training NN</a></span></li><li><span><a href=\"#4.-Embeddings-&amp;-Text-Cleaning\" data-toc-modified-id=\"4.-Embeddings-&amp;-Text-Cleaning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><strong>4. Embeddings &amp; Text Cleaning</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Embedding-Coverage\" data-toc-modified-id=\"4.1-Embedding-Coverage-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><strong>4.1 Embedding Coverage</strong></a></span></li><li><span><a href=\"#4.2-Text-Cleaning-(Not-Finished)\" data-toc-modified-id=\"4.2-Text-Cleaning-(Not-Finished)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span><strong>4.2 Text Cleaning (Not Finished)</strong></a></span></li></ul></li><li><span><a href=\"#5.-Test-Set-Labels\" data-toc-modified-id=\"5.-Test-Set-Labels-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><strong>5. Test Set Labels</strong></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:30.109462Z",
     "start_time": "2020-01-07T16:27:30.093837Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import re\n",
    "# import string\n",
    "# import operator\n",
    "# from collections import defaultdict\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from wordcloud import STOPWORDS\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:44:40.394239Z",
     "start_time": "2020-01-07T16:44:40.360774Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from random import sample, random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set_style('whitegrid')\n",
    "col1, col2 = 'rebeccapurple', 'yellow'\n",
    "\n",
    "# Display settings\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:31.666160Z",
     "start_time": "2020-01-07T16:27:31.575551Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape = (7613, 5)\n",
      "Training Set Memory Usage = 0.20 MB\n",
      "Test Set Shape = (3263, 4)\n",
      "Test Set Memory Usage = 0.08 MB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/raw/train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_test = pd.read_csv('data/raw/test.csv', dtype={'id': np.int16})\n",
    "\n",
    "print(f'Training Set Shape = {df_train.shape}')\n",
    "print(f'Training Set Memory Usage = {df_train.memory_usage().sum() / 1024**2:.2f} MB')\n",
    "print(f'Test Set Shape = {df_test.shape}')\n",
    "print(f'Test Set Memory Usage = {df_test.memory_usage().sum() / 1024**2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:31.694161Z",
     "start_time": "2020-01-07T16:27:31.666160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.head(2))\n",
    "display(df_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T15:39:12.605913Z",
     "start_time": "2020-01-07T15:39:12.565971Z"
    }
   },
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:31.718167Z",
     "start_time": "2020-01-07T16:27:31.698170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define cleaning function\n",
    "\n",
    "def clean_text(text_list, trim_threshold=5):\n",
    "    \"\"\"Clean the input text (replace punctuation, remove unfrequent words) \n",
    "    Return cleaned text as list of tweets and as one large string.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - text_list: iterable of strings\n",
    "    - trim_threshold: int, all words with <= this frequency will be removed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    - text_complete: str, cleaned text \n",
    "    - text_list: list of strings, cleaned text split into individual tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate tweets into one large string for further cleaning\n",
    "    text = \"\"\n",
    "    for item in text_list:\n",
    "        text = text + item + \" || \"\n",
    "    \n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')  \n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_Double> ')\n",
    "    text = text.replace(\"'\", ' <QUOTATION> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "#     text = text.replace('\\n', ' <NEW_LINE> ')  # watch whitespaces\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    text = text.replace('#', ' <HASHTAG> ')\n",
    "    text = text.replace('@', ' <MENTION> ')\n",
    "    \n",
    "      \n",
    "    # Remove all words with x or fewer occurences\n",
    "    words_all = text.split(\" \")\n",
    "    word_counts = Counter(words_all)\n",
    "    words_trimmed = [word for word in words_all if word_counts[word] > trim_threshold]\n",
    "    text_trimmed = ' '.join([word for word in words_trimmed])\n",
    "\n",
    "    # Split by new lines, reassemble \n",
    "    text_list = text_trimmed.split(' || ')[:-1] # last item is empty\n",
    "    text_complete = ' '.join(text_list)  \n",
    "\n",
    "    return text_complete, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.092489Z",
     "start_time": "2020-01-07T16:27:31.722166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 7613\n",
      "Number of unique words in text_complete: 3489\n"
     ]
    }
   ],
   "source": [
    "text_complete, tweets_train = clean_text(df_train['text'], 3)\n",
    "\n",
    "vocab_list = list(set(text_complete.split()))\n",
    "print(f\"Number of tweets: {len(tweets_train)}\")\n",
    "print(f\"Number of unique words in text_complete: {len(vocab_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode\n",
    "Create 2 dicts mapping words in vocabulary to integers and vice-versa.\n",
    "-  sure that most frequent words get lowest int representation\n",
    "- DON'T leave pos [0] for padding (see next section) yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.108291Z",
     "start_time": "2020-01-07T16:27:32.096474Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_text(text, start_pos=0):\n",
    "    \"\"\"Encode words to integers (and vice versa), so that most frequent words\n",
    "    get lowest int representations. Return two mapping dictionaries.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - text: string, input text with words delimited by whitespaces\n",
    "    - start_pos: int, value for first integer representation (default=0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    - vocab_to_int: dict, mapping of words to ints (most frequent comes first)\n",
    "    - int_to_vocab: dict, mapping of ints to words\n",
    "    \"\"\"\n",
    "\n",
    "    word_count_dict = Counter(text.split())\n",
    "    vocab = sorted(word_count_dict, key=word_count_dict.get, reverse=True)\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab, start_pos)}\n",
    "    int_to_vocab = {i: word for i, word in enumerate(vocab, start_pos)}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.172426Z",
     "start_time": "2020-01-07T16:27:32.111400Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = encode_text(text_complete, start_pos=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.208426Z",
     "start_time": "2020-01-07T16:27:32.176429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  3489\n",
      "[('<PERIOD>', 0), ('<COLON>', 1), ('//t', 2), ('http', 3), ('<HASHTAG>', 4), ('the', 5), ('<QUOTATION>', 6)]\n",
      "[(0, '<PERIOD>'), (1, '<COLON>'), (2, '//t'), (3, 'http'), (4, '<HASHTAG>'), (5, 'the'), (6, '<QUOTATION>')]\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "print('Unique words: ', len((vocab_to_int)))\n",
    "assert len(vocab_to_int) == len(vocab_list)\n",
    "print(list(vocab_to_int.items())[:7])\n",
    "print(list(int_to_vocab.items())[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample\n",
    "Discard some high-frequent words from our data and in return get faster training and better representations (noise reduction). (According to [Neural Information Processing Systems, paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), Mikolov et al.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.220424Z",
     "start_time": "2020-01-07T16:27:32.208426Z"
    }
   },
   "outputs": [],
   "source": [
    "def subsample(text, vocab_to_int, threshold=1e-5):\n",
    "    \"\"\"Discard some frequent words, according to the subsampling equation.\n",
    "    Return a new (reduced) list of words for training. \n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - text: string, dict, mapping of words to ints (most frequent come first)\n",
    "    - vocab_to_int: int, value for first integer representation (default=0)\n",
    "    - threshold: float, threshold for 'random discarding' a word (default=1e-5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    - words_train: list, subsampled text represented in ints instead of words\n",
    "    \"\"\" \n",
    "    \n",
    "    # Encode the whole text into a list of ints using a mapping dict\n",
    "    int_words = [vocab_to_int[word] for word in text.split()]\n",
    "    # Create dictionary of int_words, showing their frequencies\n",
    "    word_counts = Counter(int_words)  \n",
    "    \n",
    "    total_count = len(int_words)\n",
    "    freqs = {word: count / total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold / freqs[word]) for word in word_counts}\n",
    "\n",
    "    words_train = [word for word in int_words if random() < (1 - p_drop[word])]\n",
    "    \n",
    "    return words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.399055Z",
     "start_time": "2020-01-07T16:27:32.220424Z"
    }
   },
   "outputs": [],
   "source": [
    "words_train = subsample(text_complete, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.440221Z",
     "start_time": "2020-01-07T16:27:32.399055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train text: 16341\n",
      "Length initial text: 134102\n",
      "[32, 258, 199, 138, 2107, 668, 1374, 1085, 259, 1195]\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "print(\"Length train text:\", len(words_train))\n",
    "print(\"Length initial text:\", len(text_complete.split()))\n",
    "print(words_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batches¶ \n",
    "\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose , for each training word we will select randomly a number  in range , and then use  words from history and  words from the future of the current word as correct labels.\"\n",
    "(According to original [Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.456221Z",
     "start_time": "2020-01-07T16:27:32.444224Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function will be called within the next function\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    \"\"\" Get a random-length list of target words (ints) in a window around \n",
    "    an input index (the input word).\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - words: list, text represented as list of words / ints\n",
    "    - idx: int, the index of the input word in words\n",
    "    - window_size: max window to and from idx to get target words\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    - target: list, of words near the idx, the 'label' four our input\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    min_val = np.max([idx-R, 0])  # make sure no neg index occurs, not necessary for values at end\n",
    "    target = words[min_val : idx] + words[idx+1 : idx+R+1]\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.488221Z",
     "start_time": "2020-01-07T16:27:32.468226Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    \"\"\"Create a generator of word batches as a tuple (inputs, targets). It\n",
    "    grabs `batch_size` words from a words list. Then for each of those \n",
    "    batches, it gets the target words in a window.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - words: list, text represented as list of words / ints\n",
    "    - batch size: int, number of inputs to form one batch\n",
    "    - window_size: max window to and from idx to get target words\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    - x, y: lists, inputs und corresponding labels for one batch at a time\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx : idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)  # each batch x and y will be one row of values\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:32.516221Z",
     "start_time": "2020-01-07T16:27:32.492221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'D', 'D', 'D']\n",
      "y\n",
      " ['B', 'C', 'D', 'A', 'C', 'D', 'A', 'B', 'D', 'A', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "test_text = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "x, y = next(get_batches(test_text, batch_size=4, window_size=5))\n",
    "\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN for Word2Vec Embedding\n",
    "\n",
    "### Define Model Architecture and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:40:48.995167Z",
     "start_time": "2020-01-07T16:40:48.963802Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        # Define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        # Initialize both embedding tables with uniform distribution\n",
    "        # (this may help with convergence)\n",
    "        self.in_embed.weight.data.uniform_(-1, 1)\n",
    "        self.out_embed.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        input_vectors = self.in_embed(input_words)\n",
    "        return input_vectors\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        output_vectors = self.out_embed(output_words)\n",
    "        return output_vectors\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed)\"\"\"\n",
    "        if self.noise_dist is None:\n",
    "            # Sample words uniformly\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # Sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist,\n",
    "                                        batch_size * n_samples,\n",
    "                                        replacement=True)\n",
    "        \n",
    "        device = \"cuda\" if self.out_embed.weight.is_cuda else \"cpu\"\n",
    "        noise_words = noise_words.to(device)\n",
    "        noise_vectors = self.out_embed(noise_words).view(batch_size, n_samples, self.n_embed)      \n",
    "        \n",
    "        return noise_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:40:49.974916Z",
     "start_time": "2020-01-07T16:40:49.959378Z"
    }
   },
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        \n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # Correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # Incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "\n",
    "        # Negate and sum correct and noisy log-sigmoid losses\n",
    "        # Return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define validation function\n",
    "This function helps observing the model during training. It will print out the closest words to some input words using the _cosine similarity_. We will input a mix of a few common words and a few uncommon words.\n",
    "\n",
    "We can encode the validation words as vectors using the embedding table, then calculate the similarity with each word vector  in the embedding table. With the similarities, we can print out the validation words and words in our embedding table semantically similar to those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:44:59.199945Z",
     "start_time": "2020-01-07T16:44:59.158363Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the \n",
    "    embedding matrix.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - embedding: a PyTorch embedding module\n",
    "    - ...    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the cosine similarity between some random words and the embedding vectors. \n",
    "    # With the similarities, we can look at what words are close to our random words.\n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # Calculate magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # Pick N words from our ranges (0, window) and (1000, 1000+window). \n",
    "    # lower id implies more frequent words, higher id more uncommon words\n",
    "    valid_examples = np.array(sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               sample(range(1000, 1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t()) / magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:10:03.728573Z",
     "start_time": "2020-01-07T16:10:03.712943Z"
    }
   },
   "source": [
    "## Training NN\n",
    "(Training on GPU recommended, if available.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:40:52.807369Z",
     "start_time": "2020-01-07T16:40:52.778963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "# Check for a GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if not torch.cuda.is_available():\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:40:53.491029Z",
     "start_time": "2020-01-07T16:40:53.459910Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_w2v(model, words, batch_size, optimizer, criterion, epochs, print_every=1500):\n",
    "    \"\"\"Train loop with forward and backward propagation, \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    steps = 0\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # Get our input, target batches\n",
    "        for input_words, target_words in get_batches(words, batch_size):\n",
    "            steps += 1\n",
    "            inputs, targets = torch.LongTensor(input_words), \\\n",
    "                              torch.LongTensor(target_words)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # input, outpt, and noise vectors\n",
    "            input_vectors = model.forward_input(inputs)\n",
    "            output_vectors = model.forward_output(targets)\n",
    "            noise_vectors = model.forward_noise(inputs.shape[0], 5)\n",
    "\n",
    "            # negative sampling loss\n",
    "            loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if steps % print_every == 0:\n",
    "                print(\"Epoch: {}/{}\".format(e + 1, epochs))\n",
    "                print(\"Loss: \", loss.item()) # avg batch loss at this point\n",
    "                valid_examples, valid_similarities = cosine_similarity(\n",
    "                        model.in_embed, device=device)\n",
    "                _, closest_idxs = valid_similarities.topk(6)\n",
    "\n",
    "                valid_examples, closest_idxs = valid_examples.to('cpu'), \\\n",
    "                                               closest_idxs.to('cpu')\n",
    "                for ii, valid_idx in enumerate(valid_examples):\n",
    "                    closest_words = [int_to_vocab[idx.item()] \\\n",
    "                                     for idx in closest_idxs[ii]][1:]\n",
    "                    print(int_to_vocab[valid_idx.item()] \\\n",
    "                          + \" | \" + ', '.join(closest_words))\n",
    "                print(\"...\\n\")\n",
    "    \n",
    "    # Return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:40:54.144566Z",
     "start_time": "2020-01-07T16:40:54.128509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get noise distribution\n",
    "\n",
    "# Calculate new freqs after subsampling - this was not done in orig Udacity project!\n",
    "word_counts = Counter(words_train)\n",
    "total_count = len(words_train)\n",
    "freqs = {word: count / total_count for word, count in word_counts.items()}\n",
    "word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
    "unigram_dist = word_freqs / word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:46:26.935096Z",
     "start_time": "2020-01-07T16:46:26.872589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set net parameters\n",
    "embedding_dim = 300\n",
    "\n",
    "# Instantiate model and move to GPU if available\n",
    "w2v_net = SkipGramNeg(len(vocab_to_int), embedding_dim, noise_dist=noise_dist)\n",
    "w2v_net.to(device)\n",
    "\n",
    "# Set training parameters\n",
    "print_every = 500\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "criterion = NegativeSamplingLoss() \n",
    "optimizer = optim.Adam(w2v_net.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:51:22.991623Z",
     "start_time": "2020-01-07T16:46:28.432472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20\n",
      "Loss:  11.532182693481445\n",
      "don | plunging, marvel, meek, decide, english\n",
      "are | tuesday, anthrax, beû_, about, movie\n",
      "s | ban, fedex, shoes, 45, nu\n",
      "fire | 10th, bowl, reason, 4000, santa\n",
      "<EXCLAMATION_MARK> | anyone, bank, mind, ship, alberta\n",
      "one | entertainment, bell, two, christian, these\n",
      "from | black, forces, always, tcot, theory\n",
      "http | i, explosion, lucky, tube, capsizes\n",
      "wonder | captures, breaking, ferguson, radiation, okwx\n",
      "nigerian | been, careful, aba, co/moll5vd8yd, bed\n",
      "landing | week, fallen, latest, google, speaker\n",
      "share | catastrophe, kyle, nice, nothing, dick\n",
      "dad | gun, thru, trial, our, shares\n",
      "told | talk, california, reddit, mansehra, drill\n",
      "early | ross, civilians, details, earth, usually\n",
      "tried | terror, m1, that, sees, forced\n",
      "...\n",
      "\n",
      "Epoch: 4/20\n",
      "Loss:  8.58129596710205\n",
      "http | i, send, explosion, lucky, have\n",
      "me | animal, been, in], footage, wedding\n",
      "the | wearing, on, tons, exploration, but\n",
      "i | http, egypt, giving, target, predicted\n",
      "body | wins, memories, clearly, track, tons\n",
      "we | manslaughter, okay, according, landfall, hotel\n",
      "m | side, players, dk, generation, grade\n",
      "<LEFT_PAREN> | <QUOTATION>, both, rape, crash, a\n",
      "guess | 5th, floods, 33, state, 9\n",
      "series | bless, sound, allah, physical, drink\n",
      "70th | marvel, hiring, &gt, cant, virgin\n",
      "tree | fans, logo, security, potential, hands\n",
      "pradesh | blog, ep, cases, town, simple\n",
      "funtenna | 600, meek, understanding, calif, lol\n",
      "scene | ankle, department, township, deal, need\n",
      "safety | pacific, 16yr, sw, hasn, explosion\n",
      "...\n",
      "\n",
      "Epoch: 6/20\n",
      "Loss:  4.570802688598633\n",
      "more | outside, <SEMICOLON>, 16yr, one, internally\n",
      "have | <COLON>, aid, ground, business, libya\n",
      "he | is, av, chemical, wanna, accidentally\n",
      "and | cleared, re, <RIGHT_PAREN>, god, myanmar\n",
      "how | got, whole, xbox, expert, salt\n",
      "do | hurricane, treat, else, except, okay\n",
      "disaster | california, thinking, lowly, want, asian\n",
      "you | guy, especially, weapons, shares, kca\n",
      "expected | 4, here, know, pov, headed\n",
      "bayelsa | gems, houses, somalia, smoking, life\n",
      "businesses | major, justinbieber, wind, lulgzimbestpicts, flash\n",
      "issued | dogs, sat, 00, grey, pitch\n",
      "thinking | metro, disaster, recovery, mlb, tracks\n",
      "soul | been, parents, 35, declaration, trying\n",
      "pray | girlfriend, &gt, fans, tonto, jobs\n",
      "pakistani | since, cakes, 101, hazardous, impressed\n",
      "...\n",
      "\n",
      "Epoch: 8/20\n",
      "Loss:  2.260374069213867\n",
      "disaster | california, want, lowly, thinking, kick\n",
      "video | average, please, mom, chile, broad\n",
      "do | hurricane, treat, else, except, officer\n",
      "there | hearts, wreck, wonderful, keeps, country\n",
      "more | outside, <SEMICOLON>, 16yr, internally, enugu\n",
      "it | pop, dtn, gm, see, crime\n",
      "so | india, peaceful, co/o91f3cyy0r, understanding, watching\n",
      "to | canaanites, choice, japanese, shoulder, chaos\n",
      "guess | 5th, floods, buckle, 33, storms\n",
      "neighbour | floyd, really, happy, heading, detonated\n",
      "bayelsa | gems, houses, somalia, smoking, announcement\n",
      "sounds | raid, over, room, battle, train\n",
      "pradesh | cases, blog, ep, version, town\n",
      "led | earthquake, hunt, screams, ignition, waiting\n",
      "york | throw, river, flags, half, call\n",
      "unconfirmed | fires, gm, dry, breakingnews, migrants\n",
      "...\n",
      "\n",
      "Epoch: 10/20\n",
      "Loss:  2.3383452892303467\n",
      "body | memories, clearly, wins, finally, tote\n",
      "i | http, giving, egypt, deluge, hat\n",
      "be | watching, wmata, investigating, z10, could\n",
      "but | 3, deluged, disney, hot, de\n",
      "how | xbox, expert, got, 10, add\n",
      "been | found, survive, soul, arsonist, broad\n",
      "were | udhampur, wrought, confirmed, j, girls\n",
      "police | soudelor, christian, sources, warship, quite\n",
      "nagasaki | latest, vehicle, rexyy, hwy, killed\n",
      "likely | faith, occurred, moves, fedex, bell\n",
      "dog | warning, method, drove, slanglucci, co/vvplfqv58p\n",
      "download | trailer, ûó, agreed, rape, flooding\n",
      "give | thunder, once, hereûªs, share, causes\n",
      "department | b4, scene, byû_, leveled, somehow\n",
      "mode | enjoying, stuart, investigating, radar, bin\n",
      "party | outrage, without, buildings, provoke, radar\n",
      "...\n",
      "\n",
      "Epoch: 12/20\n",
      "Loss:  1.6168978214263916\n",
      "your | hollywood, burning, size, housing, sort\n",
      "have | toilet, manager, business, aid, bet\n",
      "do | hurricane, treat, degree, except, anymore\n",
      "you | kca, shares, weapons, wish, especially\n",
      "news | ft, allow, interested, muscle, writer\n",
      "what | sophie, rt, el, suffering, california\n",
      "will | trying, security, 375, anthrax, humidity\n",
      "fire | 10th, summerfate, bowl, eastern, doctor\n",
      "militants | alright, area, weird, board, root\n",
      "reported | atlanta, hollywood, man, fun, friends\n",
      "scene | ankle, 360wisenews, un, content, till\n",
      "secret | certainly, added, crush, panic, bathroom\n",
      "room | veterans, sounds, materials, experts, july\n",
      "= | wire, roll, bitch, civil, hurt\n",
      "madhya | self, elevated, chemical, blocking, when\n",
      "wonder | captures, aid, radiation, casualties, devastated\n",
      "...\n",
      "\n",
      "Epoch: 14/20\n",
      "Loss:  1.2445279359817505\n",
      "video | average, please, mom, civil, chile\n",
      "- | los, 4wd, desolation, elevated, lamp\n",
      "t | fatal, non, light, houses, beam\n",
      "<QUOTATION> | lonely, happen, reaû_, dubstep, bare\n",
      "his | greatest, v, movies, scientists, mark\n",
      "people | apocalypse, +, braking, co/moll5vd8yd, snap\n",
      "i | http, giving, egypt, asian, deluge\n",
      "me | in], design, wedding, animal, trouble\n",
      "niggas | truth, bombings, tornadoes, spanish, drake\n",
      "subreddits | ya, civilians, humidity, card, ~\n",
      "militants | alright, area, weird, board, root\n",
      "series | intensity, sound, fully, physical, wondering\n",
      "pakistani | since, cakes, 101, 36, hazardous\n",
      "tree | fans, hijacking, usgs, zionist, wither\n",
      "room | sounds, july, veterans, materials, underway\n",
      "risk | senator, turkish, bell, soul, traditional\n",
      "...\n",
      "\n",
      "Epoch: 16/20\n",
      "Loss:  1.0971158742904663\n",
      "when | ross, madhya, russian, path, +\n",
      "on | bridge, right, sh, budget, visit\n",
      "about | review, leaves, hollywood, germany, racist\n",
      "new | arrived, easily, looked, fatalities, dragon\n",
      "still | mercy, u, prevent, 300, shooting\n",
      "//t | root, respond, laws, nursing, tornadoes\n",
      "can | potus, sport, benefits, weren, some\n",
      "- | los, 4wd, lamp, help, elevated\n",
      "reported | atlanta, hollywood, pipe, asian, man\n",
      "sign | chaos, preparedness, ebola, dry, struggles\n",
      "outrage | party, 00, democracy, watch, link\n",
      "former | bulletin, travel, style, wonder, updates\n",
      "ancient | reward, adult, automatic, station, upheaval\n",
      "sounds | over, room, battle, np, raid\n",
      "potus | south, demolished, arrive, afghanistan, 5th\n",
      "soul | pro, been, parents, across, risk\n",
      "...\n",
      "\n",
      "Epoch: 18/20\n",
      "Loss:  1.1534075736999512\n",
      "after | owners, tempered, womens, fedex, gay\n",
      "<QUOTATION> | dubstep, huffman, fish, bare, happen\n",
      "all | yall, start, part, bad, picture\n",
      "<SEMICOLON> | interview, my, &gt, armageddon, hurricane\n",
      "just | i5, babies, insurer, muscle, affected\n",
      "at | telling, fears, co/cybksxhf7d, riot, residents\n",
      "on | sh, bridge, right, several, budget\n",
      "you | kca, weapons, shares, aussies, especially\n",
      "soul | across, pro, parents, been, risk\n",
      "turn | sandstorm, lorries, 60, articles, reviews\n",
      "expected | utc2015-08-05, failed, pov, writing, stuff\n",
      "working | garden, rider, deluge, straight, shipping\n",
      "galactic | idfire, flooded, ig, ha, virgin\n",
      "likely | faith, moves, unveiled, grill, san\n",
      "british | eden, description, 10, manslaughter, writer\n",
      "subreddits | alabama, ebola, civilians, card, humidity\n",
      "...\n",
      "\n",
      "Epoch: 20/20\n",
      "Loss:  0.785659909248352\n",
      "her | dutch, landslide, m1, guillermo, lil\n",
      "by | idfire, lands, hop, 12000, starring\n",
      "i | asian, giving, egypt, ofû_, create\n",
      "you | kca, weapons, shares, guy, rail\n",
      "t | non, fatal, operations, second, trump\n",
      "on | several, bridge, sh, heart, avoid\n",
      "was | helping, takes, change, shooting, prosecuted\n",
      "will | easy, humidity, 375, anthrax, prefer\n",
      "nigerian | baseball, moment, 2014, banned, arrested\n",
      "2015] | waves, understanding, bring, rocket, examining\n",
      "likely | moves, unveiled, faith, grill, san\n",
      "former | bulletin, travel, style, wonder, eq\n",
      "pray | disea, girlfriend, jobs, val, losses\n",
      "middle | delays, hiring, typhoon, load, uganda\n",
      "members | dirt, laden, wasn, pandemonium, bob\n",
      "pamela | waving, temple, geller, satchel, christian\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trained_w2v_net = train_w2v(w2v_net, words_train, batch_size, \n",
    "                            optimizer, criterion, epochs, print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:53:07.372734Z",
     "start_time": "2020-01-07T16:53:07.334730Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(w2v_net.state_dict(), 'models/w2v_net.pth')\n",
    "\n",
    "#  state_dict = torch.load('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Embeddings & Text Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Embedding Coverage**\n",
    "* When you have pre-trained embeddings, doing standard preprocessing steps  might not be a good idea because some of the valuable information can be lost. It is better to get vocabulary as close to embeddings as possible. In order to do that, `train_vocab` and `test_vocab` are created by counting the words in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:33.024224Z",
     "start_time": "2020-01-07T16:27:30.347Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(tweets):\n",
    "    vocab = {}        \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "train_tweets = df_train['text'].apply(lambda s: s.split()).values\n",
    "train_vocab = build_vocab(train_tweets)\n",
    "test_tweets = df_test['text'].apply(lambda s: s.split()).values\n",
    "test_vocab = build_vocab(test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings used for text cleaning are:\n",
    "* GloVe-300d-840B\n",
    "* FastText-Crawl-300d-2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:33.024224Z",
     "start_time": "2020-01-07T16:27:30.355Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_glove = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', allow_pickle=True)\n",
    "embeddings_fasttext = np.load('../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in intersection of vocab and embeddings are stored in `covered` along with their counts. Words in vocab that don't exist in embeddings are stored in `oov` along with their counts. `n_covered` and `n_oov` are total number of counts and they are used for calculating coverage percentages.\n",
    "\n",
    "Both GloVe and FastText embeddings have more than **50%** vocab and **80%** text coverage without cleaning. GloVe and FastText coverage are very close but GloVe has slightly higher coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:33.028224Z",
     "start_time": "2020-01-07T16:27:30.371Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings, embeddings_name, dataset_name):\n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    print('{} Embeddings cover {:.2%} of {} vocab'.format(embeddings_name, vocab_coverage, dataset_name))\n",
    "    print('{} Embeddings cover {:.2%} of {} text'.format(embeddings_name, text_coverage, dataset_name))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov\n",
    "\n",
    "train_oov_glove = check_coverage(train_vocab, embeddings_glove, 'GloVe', 'Training')\n",
    "test_oov_glove = check_coverage(test_vocab, embeddings_glove, 'GloVe', 'Test')\n",
    "train_oov_fasttext = check_coverage(train_vocab, embeddings_fasttext, 'FastText', 'Training')\n",
    "test_oov_fasttext = check_coverage(test_vocab, embeddings_fasttext, 'FastText', 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Text Cleaning (Not Finished)**\n",
    "* The most common type of words in oov have punctuations at the start or end. Those words doesn't have embeddings because of those trailing punctuations. `#@!?()[]*%...'` are separated from words.\n",
    "* There are special characters in tweets and some of them are attached to words. They are removed completely.\n",
    "* Contractions are expanded.\n",
    "* Character entity references are replaced with their actual symbols.\n",
    "* Typos and slang are corrected, and informal abbreviations are written in their long forms.\n",
    "* Hashtags and usernames are expanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:33.028224Z",
     "start_time": "2020-01-07T16:27:30.386Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(tweet):    \n",
    "    # Punctuations at the start or end of words    \n",
    "    for punctuation in \"#@!?()[]*%\":\n",
    "        tweet = tweet.replace(punctuation, f' {punctuation} ').strip()\n",
    "    tweet = tweet.replace('...', ' ... ').strip()\n",
    "    tweet = tweet.replace(\"'\", \" ' \").strip()        \n",
    "    \n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "        \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "        \n",
    "    # Hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "df_train['text_cleaned'] = df_train['text'].apply(lambda x : clean(x))\n",
    "df_test['text_cleaned'] = df_test['text'].apply(lambda x : clean(x))\n",
    "\n",
    "train_tweets_cleaned = df_train['text_cleaned'].apply(lambda s: s.split()).values\n",
    "train_vocab_cleaned = build_vocab(train_tweets_cleaned)\n",
    "test_tweets_cleaned = df_test['text_cleaned'].apply(lambda s: s.split()).values\n",
    "test_vocab_cleaned = build_vocab(test_tweets_cleaned)\n",
    "\n",
    "train_oov_glove = check_coverage(train_vocab_cleaned, embeddings_glove, 'GloVe', 'Training')\n",
    "test_oov_glove = check_coverage(test_vocab_cleaned, embeddings_glove, 'GloVe', 'Test')\n",
    "train_oov_fasttext = check_coverage(train_vocab_cleaned, embeddings_fasttext, 'FastText', 'Training')\n",
    "test_oov_fasttext = check_coverage(test_vocab_cleaned, embeddings_fasttext, 'FastText', 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Test Set Labels**\n",
    "Test set labels can be found on [this](https://www.figure-eight.com/data-for-everyone/) website. Dataset is named **Disasters on social media**. This is how people are submitting perfect scores. Other \"Getting Started\" competitions also have their test labels leaked. The main point of \"Getting Started\" competitions is **learning and sharing**, and perfect score doesn't mean anything. \n",
    "\n",
    "> **Phil Culliton wrote:**\n",
    "> For the AutoML prize, any use of the label set will result in disqualification.\n",
    "\n",
    "According to [@philculliton](https://www.kaggle.com/philculliton) from Kaggle Team, competitors who use test set labels in any way are not eligible to win AutoML prize. There are no other penalties for using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:33.032222Z",
     "start_time": "2020-01-07T16:27:30.405Z"
    }
   },
   "outputs": [],
   "source": [
    "df_leak = pd.read_csv('../input/nlp-with-disaster-tweets-test-set-with-labels/socialmedia-disaster-tweets-DFE.csv', encoding ='ISO-8859-1')[['choose_one', 'text']]\n",
    "\n",
    "# Creating target and id\n",
    "df_leak['target'] = (df_leak['choose_one'] == 'Relevant').astype(np.int8)\n",
    "df_leak['id'] = df_leak.index.astype(np.int16)\n",
    "df_leak.drop(columns=['choose_one', 'text'], inplace=True)\n",
    "\n",
    "# Merging target to test set\n",
    "df_test = df_test.merge(df_leak, on=['id'], how='left')\n",
    "\n",
    "print('Leaked Data Set Shape = {}'.format(df_leak.shape))\n",
    "print('Leaked Data Set Memory Usage = {:.2f} MB'.format(df_leak.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T16:27:33.032222Z",
     "start_time": "2020-01-07T16:27:30.420Z"
    }
   },
   "outputs": [],
   "source": [
    "ORIGINAL_FEATURES = ['id', 'keyword', 'location', 'text', 'target', 'text_cleaned']\n",
    "\n",
    "df_train[ORIGINAL_FEATURES].to_pickle('train.pkl')\n",
    "df_test[ORIGINAL_FEATURES].to_pickle('test.pkl')\n",
    "\n",
    "submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "submission['target'] = df_test['target'].values\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Training Set Shape = {}'.format(df_train[ORIGINAL_FEATURES].shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train[ORIGINAL_FEATURES].memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test[ORIGINAL_FEATURES].shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test[ORIGINAL_FEATURES].memory_usage().sum() / 1024**2))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
